{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3sw0CgvJw0i",
        "outputId": "d6e7e35e-4b3d-42a1-b858-00acd896492e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - accuracy: 0.9245 - loss: 0.4086 - val_accuracy: 0.9960 - val_loss: 0.0724\n",
            "Epoch 2/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9980 - loss: 0.0611 - val_accuracy: 1.0000 - val_loss: 0.0137\n",
            "Epoch 3/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9987 - loss: 0.0208 - val_accuracy: 1.0000 - val_loss: 0.0061\n",
            "Epoch 4/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9985 - loss: 0.0115 - val_accuracy: 1.0000 - val_loss: 0.0029\n",
            "Epoch 5/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9994 - loss: 0.0074 - val_accuracy: 1.0000 - val_loss: 0.0035\n",
            "Epoch 6/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 0.9990 - val_loss: 0.0019\n",
            "Epoch 7/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9984 - loss: 0.0048 - val_accuracy: 1.0000 - val_loss: 8.3796e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9977 - loss: 0.0092 - val_accuracy: 0.9990 - val_loss: 0.0018\n",
            "Epoch 9/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9997 - loss: 0.0020 - val_accuracy: 1.0000 - val_loss: 5.4413e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 1.0000 - val_loss: 3.2036e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9949 - loss: 0.0178 - val_accuracy: 1.0000 - val_loss: 4.0117e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9994 - loss: 0.0024 - val_accuracy: 0.9980 - val_loss: 0.0028\n",
            "Epoch 13/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0018 - val_accuracy: 1.0000 - val_loss: 2.6326e-04\n",
            "Epoch 14/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9997 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 1.6470e-04\n",
            "Epoch 15/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.9111e-04 - val_accuracy: 1.0000 - val_loss: 1.2339e-04\n",
            "Epoch 16/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9999 - loss: 6.6019e-04 - val_accuracy: 1.0000 - val_loss: 1.5536e-04\n",
            "Epoch 17/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 5.2058e-04 - val_accuracy: 1.0000 - val_loss: 8.3074e-05\n",
            "Epoch 18/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 5.0127e-04 - val_accuracy: 1.0000 - val_loss: 6.4339e-05\n",
            "Epoch 19/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 3.1061e-04 - val_accuracy: 1.0000 - val_loss: 5.6218e-05\n",
            "Epoch 20/20\n",
            "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9998 - loss: 8.9763e-04 - val_accuracy: 0.9970 - val_loss: 0.0101\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9951 - loss: 0.0173\n",
            "Test Accuracy: 99.70%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic Bluetooth log data\n",
        "num_samples = 5000  # Total logs\n",
        "attack_ratio = 0.1  # 10% attack logs\n",
        "\n",
        "# Features: Device ID, Connection Duration, Pairing Attempts, Unauthorized Access, RSSI Signal Strength\n",
        "device_ids = np.random.randint(1000, 2000, num_samples)\n",
        "connection_duration = np.random.randint(1, 300, num_samples)  # in seconds\n",
        "pairing_attempts = np.random.randint(1, 5, num_samples)\n",
        "unauthorized_access = np.random.choice([0, 1], size=num_samples, p=[0.95, 0.05])\n",
        "rssi_signal = np.random.uniform(-90, -30, num_samples)  # RSSI (dBm)\n",
        "\n",
        "# Generate attack logs (high pairing attempts, unauthorized access, abnormal connection time)\n",
        "attack_indices = np.random.choice(range(num_samples), int(num_samples * attack_ratio), replace=False)\n",
        "pairing_attempts[attack_indices] = np.random.randint(5, 15, len(attack_indices))\n",
        "unauthorized_access[attack_indices] = 1\n",
        "connection_duration[attack_indices] = np.random.randint(300, 600, len(attack_indices))\n",
        "\n",
        "# Labels: 0 (Normal), 1 (Attack)\n",
        "labels = np.zeros(num_samples)\n",
        "labels[attack_indices] = 1\n",
        "\n",
        "# Create DataFrame\n",
        "bluetooth_logs = pd.DataFrame({\n",
        "    'Device_ID': device_ids,\n",
        "    'Connection_Duration': connection_duration,\n",
        "    'Pairing_Attempts': pairing_attempts,\n",
        "    'Unauthorized_Access': unauthorized_access,\n",
        "    'RSSI_Signal': rssi_signal,\n",
        "    'Label': labels  # 0 = Normal, 1 = Attack\n",
        "})\n",
        "\n",
        "# Select features and labels\n",
        "features = ['Connection_Duration', 'Pairing_Attempts', 'Unauthorized_Access', 'RSSI_Signal']\n",
        "X = bluetooth_logs[features].values\n",
        "y = bluetooth_logs['Label'].values\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape for Transformer (samples, timesteps, features)\n",
        "X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define Transformer block\n",
        "def transformer_block(inputs, num_heads=4, ff_dim=32, dropout=0.1):\n",
        "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)(inputs, inputs)\n",
        "    attn_output = Dropout(dropout)(attn_output)\n",
        "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
        "\n",
        "    ffn_output = Dense(ff_dim, activation='relu')(out1)\n",
        "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
        "    ffn_output = Dropout(dropout)(ffn_output)\n",
        "    out2 = LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
        "\n",
        "    return out2\n",
        "\n",
        "# Model Input\n",
        "inputs = Input(shape=(1, X_reshaped.shape[2]))\n",
        "x = transformer_block(inputs)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dense(25, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Build Model\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdKIEa2uKCRF",
        "outputId": "8e768543-e35e-456b-8904-0e43f3787c2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 9002 entries, 0 to 9001\n",
            "Data columns (total 5 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   Protocol  9002 non-null   object \n",
            " 1   Info      9002 non-null   object \n",
            " 2   Length    9002 non-null   int64  \n",
            " 3   Delta     9002 non-null   float64\n",
            " 4   Type      9002 non-null   object \n",
            "dtypes: float64(1), int64(1), object(3)\n",
            "memory usage: 351.8+ KB\n",
            "Epoch 1/20\n",
            "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.3721 - loss: 2.0856 - val_accuracy: 0.6891 - val_loss: 1.0926\n",
            "Epoch 2/20\n",
            "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6952 - loss: 1.1307 - val_accuracy: 0.6891 - val_loss: 1.0550\n",
            "Epoch 3/20\n",
            "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6944 - loss: 1.0893 - val_accuracy: 0.6891 - val_loss: 1.0512\n",
            "Epoch 4/20\n",
            "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6946 - loss: 1.0865 - val_accuracy: 0.6891 - val_loss: 1.0513\n",
            "Epoch 5/20\n",
            "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7029 - loss: 1.0606 - val_accuracy: 0.6891 - val_loss: 1.0508\n",
            "Epoch 6/20\n",
            "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6961 - loss: 1.0841 - val_accuracy: 0.6891 - val_loss: 1.0508\n",
            "Epoch 7/20\n",
            "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.6899 - loss: 1.0889 - val_accuracy: 0.6891 - val_loss: 1.0506\n",
            "Epoch 8/20\n",
            "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.6937 - loss: 1.0871 - val_accuracy: 0.6891 - val_loss: 1.0532\n",
            "Epoch 9/20\n",
            "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7011 - loss: 1.0631 - val_accuracy: 0.6891 - val_loss: 1.0510\n",
            "Epoch 10/20\n",
            "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7014 - loss: 1.0547 - val_accuracy: 0.6891 - val_loss: 1.0506\n",
            "Epoch 11/20\n",
            "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6960 - loss: 1.0671 - val_accuracy: 0.6891 - val_loss: 1.0503\n",
            "Epoch 12/20\n",
            "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6971 - loss: 1.0658 - val_accuracy: 0.6891 - val_loss: 1.0515\n",
            "Epoch 13/20\n",
            "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6978 - loss: 1.0734 - val_accuracy: 0.6891 - val_loss: 1.0530\n",
            "Epoch 14/20\n",
            "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6978 - loss: 1.0711 - val_accuracy: 0.6891 - val_loss: 1.0504\n",
            "Epoch 15/20\n",
            "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7042 - loss: 1.0616 - val_accuracy: 0.6891 - val_loss: 1.0509\n",
            "Epoch 16/20\n",
            "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.6940 - loss: 1.0739 - val_accuracy: 0.6891 - val_loss: 1.0504\n",
            "Epoch 17/20\n",
            "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.6936 - loss: 1.0734 - val_accuracy: 0.6891 - val_loss: 1.0524\n",
            "Epoch 18/20\n",
            "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7000 - loss: 1.0627 - val_accuracy: 0.6891 - val_loss: 1.0516\n",
            "Epoch 19/20\n",
            "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7122 - loss: 1.0213 - val_accuracy: 0.6891 - val_loss: 1.0505\n",
            "Epoch 20/20\n",
            "\u001b[1m226/226\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6965 - loss: 1.0679 - val_accuracy: 0.6891 - val_loss: 1.0517\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6800 - loss: 1.0759\n",
            "Test Accuracy: 68.91%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "\n",
        "# Load BrakTooth Attack Dataset\n",
        "file_path = 'BrakTooth_Dataset.xlsx'  # Update with actual file path\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Display dataset info\n",
        "data.info()\n",
        "\n",
        "# Select relevant features based on dataset columns\n",
        "features = ['Length', 'Delta']  # Using numerical features\n",
        "label_column = 'Type'  # Assuming 'Type' is the attack label\n",
        "\n",
        "# Extract feature matrix and labels\n",
        "X = data[features].values\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(data[label_column])\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape for Transformer (samples, timesteps, features)\n",
        "X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define Transformer block\n",
        "def transformer_block(inputs, num_heads=4, ff_dim=32, dropout=0.1):\n",
        "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)(inputs, inputs)\n",
        "    attn_output = Dropout(dropout)(attn_output)\n",
        "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
        "\n",
        "    ffn_output = Dense(ff_dim, activation='relu')(out1)\n",
        "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
        "    ffn_output = Dropout(dropout)(ffn_output)\n",
        "    out2 = LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
        "\n",
        "    return out2\n",
        "\n",
        "# Model Input\n",
        "inputs = Input(shape=(1, X_reshaped.shape[2]))\n",
        "x = transformer_block(inputs)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dense(25, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "outputs = Dense(len(label_encoder.classes_), activation='softmax')(x)  # Multi-class classification\n",
        "\n",
        "# Build Model\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSsW9gj9NeQn",
        "outputId": "6d123e64-6782-4fb3-b290-4d0e52aae921"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 9002 entries, 0 to 9001\n",
            "Data columns (total 5 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   Protocol  9002 non-null   object \n",
            " 1   Info      9002 non-null   object \n",
            " 2   Length    9002 non-null   int64  \n",
            " 3   Delta     9002 non-null   float64\n",
            " 4   Type      9002 non-null   object \n",
            "dtypes: float64(1), int64(1), object(3)\n",
            "memory usage: 351.8+ KB\n",
            "Epoch 1/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 38ms/step - accuracy: 0.5394 - loss: 1.6462 - val_accuracy: 0.6891 - val_loss: 1.0803\n",
            "Epoch 2/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.6892 - loss: 1.0929 - val_accuracy: 0.6891 - val_loss: 1.0674\n",
            "Epoch 3/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.6995 - loss: 1.0597 - val_accuracy: 0.6891 - val_loss: 1.0531\n",
            "Epoch 4/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.6938 - loss: 1.0674 - val_accuracy: 0.6891 - val_loss: 1.0538\n",
            "Epoch 5/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.6955 - loss: 1.0566 - val_accuracy: 0.6891 - val_loss: 1.0503\n",
            "Epoch 6/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - accuracy: 0.6922 - loss: 1.0622 - val_accuracy: 0.6880 - val_loss: 1.0523\n",
            "Epoch 7/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.6932 - loss: 1.0495 - val_accuracy: 0.6880 - val_loss: 1.0501\n",
            "Epoch 8/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.6958 - loss: 1.0604 - val_accuracy: 0.6880 - val_loss: 1.0492\n",
            "Epoch 9/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.6946 - loss: 1.0452 - val_accuracy: 0.6880 - val_loss: 1.0551\n",
            "Epoch 10/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.6954 - loss: 1.0481 - val_accuracy: 0.6880 - val_loss: 1.0490\n",
            "Epoch 11/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - accuracy: 0.6939 - loss: 1.0464 - val_accuracy: 0.6880 - val_loss: 1.0471\n",
            "Epoch 12/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - accuracy: 0.7025 - loss: 1.0241 - val_accuracy: 0.6880 - val_loss: 1.0465\n",
            "Epoch 13/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7016 - loss: 1.0229 - val_accuracy: 0.6880 - val_loss: 1.0622\n",
            "Epoch 14/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.7073 - loss: 1.0131 - val_accuracy: 0.6874 - val_loss: 1.0322\n",
            "Epoch 15/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.6990 - loss: 1.0220 - val_accuracy: 0.6874 - val_loss: 1.0280\n",
            "Epoch 16/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - accuracy: 0.7046 - loss: 0.9961 - val_accuracy: 0.6874 - val_loss: 1.0212\n",
            "Epoch 17/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7013 - loss: 0.9917 - val_accuracy: 0.6874 - val_loss: 1.0342\n",
            "Epoch 18/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.7044 - loss: 0.9799 - val_accuracy: 0.7035 - val_loss: 0.9823\n",
            "Epoch 19/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.6976 - loss: 0.9902 - val_accuracy: 0.6857 - val_loss: 0.9884\n",
            "Epoch 20/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.7031 - loss: 0.9680 - val_accuracy: 0.6896 - val_loss: 1.0160\n",
            "Epoch 21/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.7054 - loss: 0.9750 - val_accuracy: 0.7057 - val_loss: 0.9578\n",
            "Epoch 22/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7088 - loss: 0.9582 - val_accuracy: 0.6952 - val_loss: 0.9634\n",
            "Epoch 23/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7061 - loss: 0.9662 - val_accuracy: 0.7035 - val_loss: 0.9705\n",
            "Epoch 24/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.7196 - loss: 0.9354 - val_accuracy: 0.6957 - val_loss: 0.9503\n",
            "Epoch 25/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 0.7132 - loss: 0.9477 - val_accuracy: 0.7179 - val_loss: 0.9440\n",
            "Epoch 26/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7135 - loss: 0.9451 - val_accuracy: 0.5991 - val_loss: 1.1516\n",
            "Epoch 27/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.7149 - loss: 0.9410 - val_accuracy: 0.6963 - val_loss: 1.0323\n",
            "Epoch 28/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.7110 - loss: 0.9410 - val_accuracy: 0.7190 - val_loss: 0.9384\n",
            "Epoch 29/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.7140 - loss: 0.9280 - val_accuracy: 0.7135 - val_loss: 0.9435\n",
            "Epoch 30/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.7126 - loss: 0.9341 - val_accuracy: 0.7129 - val_loss: 0.9268\n",
            "Epoch 31/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.7155 - loss: 0.9328 - val_accuracy: 0.7152 - val_loss: 0.9498\n",
            "Epoch 32/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 20ms/step - accuracy: 0.7254 - loss: 0.9045 - val_accuracy: 0.5941 - val_loss: 1.1463\n",
            "Epoch 33/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.7136 - loss: 0.9294 - val_accuracy: 0.7129 - val_loss: 0.9239\n",
            "Epoch 34/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - accuracy: 0.7143 - loss: 0.9330 - val_accuracy: 0.7190 - val_loss: 0.9215\n",
            "Epoch 35/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.7298 - loss: 0.8912 - val_accuracy: 0.7190 - val_loss: 0.9171\n",
            "Epoch 36/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7245 - loss: 0.9029 - val_accuracy: 0.7190 - val_loss: 0.9123\n",
            "Epoch 37/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7162 - loss: 0.9170 - val_accuracy: 0.7152 - val_loss: 0.9239\n",
            "Epoch 38/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.7197 - loss: 0.9153 - val_accuracy: 0.7129 - val_loss: 0.9177\n",
            "Epoch 39/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.7087 - loss: 0.9378 - val_accuracy: 0.7096 - val_loss: 0.9314\n",
            "Epoch 40/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 30ms/step - accuracy: 0.7155 - loss: 0.9218 - val_accuracy: 0.7185 - val_loss: 0.9118\n",
            "Epoch 41/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - accuracy: 0.7208 - loss: 0.9070 - val_accuracy: 0.6929 - val_loss: 0.9693\n",
            "Epoch 42/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.7114 - loss: 0.9291 - val_accuracy: 0.7135 - val_loss: 0.9218\n",
            "Epoch 43/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.7263 - loss: 0.8933 - val_accuracy: 0.7035 - val_loss: 0.9538\n",
            "Epoch 44/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.7214 - loss: 0.9116 - val_accuracy: 0.7185 - val_loss: 0.9115\n",
            "Epoch 45/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.7213 - loss: 0.8981 - val_accuracy: 0.7190 - val_loss: 0.9177\n",
            "Epoch 46/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.7216 - loss: 0.8997 - val_accuracy: 0.7190 - val_loss: 0.9111\n",
            "Epoch 47/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.7220 - loss: 0.8997 - val_accuracy: 0.7196 - val_loss: 0.9035\n",
            "Epoch 48/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - accuracy: 0.7214 - loss: 0.8964 - val_accuracy: 0.7190 - val_loss: 0.9108\n",
            "Epoch 49/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - accuracy: 0.7288 - loss: 0.8802 - val_accuracy: 0.7185 - val_loss: 0.9146\n",
            "Epoch 50/50\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.7193 - loss: 0.9002 - val_accuracy: 0.7129 - val_loss: 0.9395\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7061 - loss: 0.9483\n",
            "Test Accuracy: 71.29%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D, Conv1D, BatchNormalization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "\n",
        "# Load BrakTooth Attack Dataset\n",
        "file_path = 'BrakTooth_Dataset.xlsx'  # Update with actual file path\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Display dataset info\n",
        "data.info()\n",
        "\n",
        "# Select relevant features based on dataset columns\n",
        "features = ['Length', 'Delta']  # Using numerical features\n",
        "label_column = 'Type'  # Assuming 'Type' is the attack label\n",
        "\n",
        "# Extract feature matrix and labels\n",
        "X = data[features].values\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(data[label_column])\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape for Transformer (samples, timesteps, features)\n",
        "X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define Transformer block\n",
        "def transformer_block(inputs, num_heads=8, ff_dim=64, dropout=0.2):\n",
        "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)(inputs, inputs)\n",
        "    attn_output = Dropout(dropout)(attn_output)\n",
        "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
        "\n",
        "    ffn_output = Dense(ff_dim, activation='relu')(out1)\n",
        "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
        "    ffn_output = Dropout(dropout)(ffn_output)\n",
        "    out2 = LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
        "\n",
        "    return out2\n",
        "\n",
        "# Model Input\n",
        "inputs = Input(shape=(1, X_reshaped.shape[2]))\n",
        "x = Conv1D(filters=32, kernel_size=1, activation='relu')(inputs)  # Added CNN layer for feature extraction\n",
        "x = BatchNormalization()(x)\n",
        "x = transformer_block(x)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dense(50, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "outputs = Dense(len(label_encoder.classes_), activation='softmax')(x)  # Multi-class classification\n",
        "\n",
        "# Build Model\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile model with improved optimizer and learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model with increased epochs and batch size\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpsqHZ8xRQ8o",
        "outputId": "de32ca1e-82e9-4e2a-c5ee-b597eef15310"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyshark\n",
            "  Downloading pyshark-0.6-py3-none-any.whl.metadata (806 bytes)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from pyshark) (5.3.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from pyshark) (2.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pyshark) (24.2)\n",
            "Collecting appdirs (from pyshark)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Downloading pyshark-0.6-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: appdirs, pyshark\n",
            "Successfully installed appdirs-1.4.4 pyshark-0.6\n"
          ]
        }
      ],
      "source": [
        "!pip install pyshark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS6IMBtERjNf",
        "outputId": "f0d07792-a37b-47dd-89fe-2195d62c1581"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfYoGzmpRrQt",
        "outputId": "beb405bc-7f6a-487a-987a-98c324ddd7f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/Bluebugging.zip\n",
            "   creating: /content/Bluebugging/\n",
            "   creating: /content/Bluebugging/train/\n",
            "  inflating: /content/Bluebugging/train/Bluetooth_Benign_train.pcap  \n",
            "  inflating: /content/Bluebugging/train/Bluetooth_DoS_train.pcap  \n",
            "   creating: /content/Bluebugging/test/\n",
            "  inflating: /content/Bluebugging/test/Bluetooth_Benign_test.pcap  \n",
            "  inflating: /content/Bluebugging/test/Bluetooth_DoS_test.pcap  \n"
          ]
        }
      ],
      "source": [
        "!unzip \"/content/drive/My Drive/Bluebugging.zip\" -d \"/content/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06RtG80gYjIK",
        "outputId": "b9537af9-322c-4ffb-a5e3-a552e7c90284"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,784 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,239 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,538 kB]\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,381 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [47.7 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,884 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,041 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,733 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,686 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,043 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Fetched 29.9 MB in 5s (6,237 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libbcg729-0 libc-ares2 liblua5.2-0 libnl-genl-3-200 libpcap0.8 libsbc1 libsmi2ldbl libspandsp2\n",
            "  libspeexdsp1 libwireshark-data libwireshark15 libwiretap12 libwsutil13 wireshark-common\n",
            "Suggested packages:\n",
            "  snmp-mibs-downloader geoipupdate geoip-database geoip-database-extra libjs-leaflet\n",
            "  libjs-leaflet.markercluster wireshark-doc\n",
            "The following NEW packages will be installed:\n",
            "  libbcg729-0 libc-ares2 liblua5.2-0 libnl-genl-3-200 libpcap0.8 libsbc1 libsmi2ldbl libspandsp2\n",
            "  libspeexdsp1 libwireshark-data libwireshark15 libwiretap12 libwsutil13 tshark wireshark-common\n",
            "0 upgraded, 15 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 23.0 MB of archives.\n",
            "After this operation, 120 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpcap0.8 amd64 1.10.1-4ubuntu1.22.04.1 [145 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libbcg729-0 amd64 1.1.1-2 [32.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 liblua5.2-0 amd64 5.2.4-2 [125 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libnl-genl-3-200 amd64 3.5.0-0.1 [12.4 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libsmi2ldbl amd64 0.4.8+dfsg2-16 [100 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libspandsp2 amd64 0.0.6+dfsg-2 [272 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libspeexdsp1 amd64 1.2~rc1.2-1.1ubuntu3 [42.5 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libwireshark-data all 3.6.2-2 [1,647 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libc-ares2 amd64 1.18.1-1ubuntu0.22.04.3 [45.1 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsbc1 amd64 1.5-3build2 [34.4 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libwsutil13 amd64 3.6.2-2 [99.2 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libwiretap12 amd64 3.6.2-2 [255 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libwireshark15 amd64 3.6.2-2 [19.5 MB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/universe amd64 wireshark-common amd64 3.6.2-2 [473 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tshark amd64 3.6.2-2 [157 kB]\n",
            "Fetched 23.0 MB in 4s (6,465 kB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package libpcap0.8:amd64.\n",
            "(Reading database ... 126209 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libpcap0.8_1.10.1-4ubuntu1.22.04.1_amd64.deb ...\n",
            "Unpacking libpcap0.8:amd64 (1.10.1-4ubuntu1.22.04.1) ...\n",
            "Selecting previously unselected package libbcg729-0:amd64.\n",
            "Preparing to unpack .../01-libbcg729-0_1.1.1-2_amd64.deb ...\n",
            "Unpacking libbcg729-0:amd64 (1.1.1-2) ...\n",
            "Selecting previously unselected package liblua5.2-0:amd64.\n",
            "Preparing to unpack .../02-liblua5.2-0_5.2.4-2_amd64.deb ...\n",
            "Unpacking liblua5.2-0:amd64 (5.2.4-2) ...\n",
            "Selecting previously unselected package libnl-genl-3-200:amd64.\n",
            "Preparing to unpack .../03-libnl-genl-3-200_3.5.0-0.1_amd64.deb ...\n",
            "Unpacking libnl-genl-3-200:amd64 (3.5.0-0.1) ...\n",
            "Selecting previously unselected package libsmi2ldbl:amd64.\n",
            "Preparing to unpack .../04-libsmi2ldbl_0.4.8+dfsg2-16_amd64.deb ...\n",
            "Unpacking libsmi2ldbl:amd64 (0.4.8+dfsg2-16) ...\n",
            "Selecting previously unselected package libspandsp2:amd64.\n",
            "Preparing to unpack .../05-libspandsp2_0.0.6+dfsg-2_amd64.deb ...\n",
            "Unpacking libspandsp2:amd64 (0.0.6+dfsg-2) ...\n",
            "Selecting previously unselected package libspeexdsp1:amd64.\n",
            "Preparing to unpack .../06-libspeexdsp1_1.2~rc1.2-1.1ubuntu3_amd64.deb ...\n",
            "Unpacking libspeexdsp1:amd64 (1.2~rc1.2-1.1ubuntu3) ...\n",
            "Selecting previously unselected package libwireshark-data.\n",
            "Preparing to unpack .../07-libwireshark-data_3.6.2-2_all.deb ...\n",
            "Unpacking libwireshark-data (3.6.2-2) ...\n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "Preparing to unpack .../08-libc-ares2_1.18.1-1ubuntu0.22.04.3_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Selecting previously unselected package libsbc1:amd64.\n",
            "Preparing to unpack .../09-libsbc1_1.5-3build2_amd64.deb ...\n",
            "Unpacking libsbc1:amd64 (1.5-3build2) ...\n",
            "Selecting previously unselected package libwsutil13:amd64.\n",
            "Preparing to unpack .../10-libwsutil13_3.6.2-2_amd64.deb ...\n",
            "Unpacking libwsutil13:amd64 (3.6.2-2) ...\n",
            "Selecting previously unselected package libwiretap12:amd64.\n",
            "Preparing to unpack .../11-libwiretap12_3.6.2-2_amd64.deb ...\n",
            "Unpacking libwiretap12:amd64 (3.6.2-2) ...\n",
            "Selecting previously unselected package libwireshark15:amd64.\n",
            "Preparing to unpack .../12-libwireshark15_3.6.2-2_amd64.deb ...\n",
            "Unpacking libwireshark15:amd64 (3.6.2-2) ...\n",
            "Selecting previously unselected package wireshark-common.\n",
            "Preparing to unpack .../13-wireshark-common_3.6.2-2_amd64.deb ...\n",
            "Unpacking wireshark-common (3.6.2-2) ...\n",
            "Selecting previously unselected package tshark.\n",
            "Preparing to unpack .../14-tshark_3.6.2-2_amd64.deb ...\n",
            "Unpacking tshark (3.6.2-2) ...\n",
            "Setting up libsbc1:amd64 (1.5-3build2) ...\n",
            "Setting up libbcg729-0:amd64 (1.1.1-2) ...\n",
            "Setting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Setting up libspandsp2:amd64 (0.0.6+dfsg-2) ...\n",
            "Setting up libsmi2ldbl:amd64 (0.4.8+dfsg2-16) ...\n",
            "Setting up libwsutil13:amd64 (3.6.2-2) ...\n",
            "Setting up libpcap0.8:amd64 (1.10.1-4ubuntu1.22.04.1) ...\n",
            "Setting up libwiretap12:amd64 (3.6.2-2) ...\n",
            "Setting up libwireshark-data (3.6.2-2) ...\n",
            "Setting up liblua5.2-0:amd64 (5.2.4-2) ...\n",
            "Setting up libnl-genl-3-200:amd64 (3.5.0-0.1) ...\n",
            "Setting up libspeexdsp1:amd64 (1.2~rc1.2-1.1ubuntu3) ...\n",
            "Setting up libwireshark15:amd64 (3.6.2-2) ...\n",
            "Setting up wireshark-common (3.6.2-2) ...\n",
            "Setting up tshark (3.6.2-2) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for shared-mime-info (2.1-2) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y tshark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWDJ6CH2Y1Vy"
      },
      "outputs": [],
      "source": [
        "!setcap 'CAP_NET_RAW+eip CAP_NET_ADMIN+eip' /usr/bin/dumpcap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-2yJOVhY9Mu",
        "outputId": "aa418140-16a6-4ffb-9d14-12b0f893cca7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running as user \"root\" and group \"root\". This could be dangerous.\n",
            "TShark (Wireshark) 3.6.2 (Git v3.6.2 packaged as 3.6.2-2)\n",
            "\n",
            "Copyright 1998-2022 Gerald Combs <gerald@wireshark.org> and contributors.\n",
            "License GPLv2+: GNU GPL version 2 or later <https://www.gnu.org/licenses/gpl-2.0.html>\n",
            "This is free software; see the source for copying conditions. There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n",
            "Compiled (64-bit) using GCC 11.2.0, with libpcap, with POSIX capabilities\n",
            "(Linux), with libnl 3, with GLib 2.71.2, with zlib 1.2.11, with Lua 5.2.4, with\n",
            "GnuTLS 3.7.3 and PKCS #11 support, with Gcrypt 1.9.4, with MIT Kerberos, with\n",
            "MaxMind DB resolver, with nghttp2 1.43.0, with brotli, with LZ4, with Zstandard,\n",
            "with Snappy, with libxml2 2.9.12, with libsmi 0.4.8.\n",
            "\n",
            "Running on Linux 6.1.85+, with Intel(R) Xeon(R) CPU @ 2.20GHz (with SSE4.2),\n",
            "with 12978 MB of physical memory, with GLib 2.72.4, with zlib 1.2.11, with\n",
            "libpcap 1.10.1 (with TPACKET_V3), with c-ares 1.18.1, with GnuTLS 3.7.3, with\n",
            "Gcrypt 1.9.4, with nghttp2 1.43.0, with brotli 1.0.9, with LZ4 1.9.3, with\n",
            "Zstandard 1.4.8, with libsmi 0.4.8, with LC_TYPE=en_US.UTF-8, binary plugins\n",
            "supported (0 loaded).\n"
          ]
        }
      ],
      "source": [
        "!tshark -v\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUp8sffawAfW",
        "outputId": "ac4eb42c-30ae-449e-b4bd-5217f5c9b201"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m18999/18999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 16ms/step - accuracy: 0.9944 - loss: 0.0227 - val_accuracy: 0.7941 - val_loss: 1.8883\n",
            "Epoch 2/50\n",
            "\u001b[1m18999/18999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 15ms/step - accuracy: 0.9993 - loss: 0.0045 - val_accuracy: 0.7941 - val_loss: 1.2718\n",
            "Epoch 3/50\n",
            "\u001b[1m18999/18999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 16ms/step - accuracy: 0.9995 - loss: 0.0034 - val_accuracy: 0.7940 - val_loss: 1.1927\n",
            "Epoch 4/50\n",
            "\u001b[1m18999/18999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 15ms/step - accuracy: 0.9995 - loss: 0.0033 - val_accuracy: 0.8585 - val_loss: 0.7142\n",
            "Epoch 5/50\n",
            "\u001b[1m18999/18999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 15ms/step - accuracy: 0.9996 - loss: 0.0012 - val_accuracy: 0.9788 - val_loss: 0.3689\n",
            "Epoch 6/50\n",
            "\u001b[1m18999/18999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 15ms/step - accuracy: 0.9998 - loss: 8.8655e-04 - val_accuracy: 0.9789 - val_loss: 0.3745\n",
            "Epoch 7/50\n",
            "\u001b[1m18999/18999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 15ms/step - accuracy: 0.9999 - loss: 7.6121e-04 - val_accuracy: 0.7940 - val_loss: 2.6772\n",
            "Epoch 8/50\n",
            "\u001b[1m18999/18999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 15ms/step - accuracy: 0.9998 - loss: 8.0941e-04 - val_accuracy: 0.9789 - val_loss: 0.3861\n",
            "Epoch 9/50\n",
            "\u001b[1m18999/18999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 16ms/step - accuracy: 0.9998 - loss: 8.6160e-04 - val_accuracy: 0.9789 - val_loss: 0.4121\n",
            "Epoch 10/50\n",
            "\u001b[1m18999/18999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 15ms/step - accuracy: 0.9999 - loss: 5.5557e-04 - val_accuracy: 0.9789 - val_loss: 0.4468\n",
            "Epoch 11/50\n",
            "\u001b[1m18999/18999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 16ms/step - accuracy: 0.9999 - loss: 7.6670e-04 - val_accuracy: 0.9789 - val_loss: 0.5152\n",
            "Epoch 12/50\n",
            "\u001b[1m18999/18999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 3.8199e-04 - val_accuracy: 0.9789 - val_loss: 0.4703\n",
            "Epoch 13/50\n",
            "\u001b[1m15942/18999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m43s\u001b[0m 14ms/step - accuracy: 0.9999 - loss: 5.1378e-04"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3431f944945c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;31m# Train model with increased epochs and batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_reshaped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;31m# Evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D, Conv1D, BatchNormalization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "import pyshark\n",
        "import os\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Function to extract features from PCAP file\n",
        "def extract_pcap_features(pcap_file, label):\n",
        "    cap = pyshark.FileCapture(pcap_file, display_filter=\"bluetooth\")\n",
        "    data = []\n",
        "\n",
        "    for pkt in cap:\n",
        "        try:\n",
        "            protocol = pkt.highest_layer\n",
        "            length = int(pkt.length)\n",
        "            timestamp = float(pkt.sniff_time.timestamp())\n",
        "\n",
        "            data.append([protocol, length, timestamp, label])\n",
        "        except AttributeError:\n",
        "            continue\n",
        "\n",
        "    cap.close()\n",
        "    return pd.DataFrame(data, columns=['Protocol', 'Length', 'Timestamp', 'Label'])\n",
        "\n",
        "# Define dataset paths\n",
        "data_folder = \"/content/Bluebugging/\"\n",
        "train_files = {\n",
        "    \"Benign\": os.path.join(data_folder, \"train\", \"Bluetooth_Benign_train.pcap\"),\n",
        "    \"Attack\": os.path.join(data_folder, \"train\", \"Bluetooth_DoS_train.pcap\")\n",
        "}\n",
        "test_files = {\n",
        "    \"Benign\": os.path.join(data_folder, \"test\", \"Bluetooth_Benign_test.pcap\"),\n",
        "    \"Attack\": os.path.join(data_folder, \"test\", \"Bluetooth_DoS_test.pcap\")\n",
        "}\n",
        "\n",
        "# Process train data\n",
        "data_train = pd.concat([\n",
        "    extract_pcap_features(train_files[\"Benign\"], \"Benign\"),\n",
        "    extract_pcap_features(train_files[\"Attack\"], \"Attack\")\n",
        "])\n",
        "\n",
        "# Process test data\n",
        "data_test = pd.concat([\n",
        "    extract_pcap_features(test_files[\"Benign\"], \"Benign\"),\n",
        "    extract_pcap_features(test_files[\"Attack\"], \"Attack\")\n",
        "])\n",
        "\n",
        "# Select relevant features\n",
        "features = ['Length', 'Timestamp']  # Numerical features\n",
        "label_column = 'Label'  # Classification target\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "data_train[label_column] = label_encoder.fit_transform(data_train[label_column])\n",
        "data_test[label_column] = label_encoder.transform(data_test[label_column])\n",
        "\n",
        "# Extract feature matrix and labels\n",
        "X_train = data_train[features].values\n",
        "y_train = data_train[label_column].values\n",
        "X_test = data_test[features].values\n",
        "y_test = data_test[label_column].values\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Reshape for Transformer (samples, timesteps, features)\n",
        "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
        "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
        "\n",
        "# Define Transformer block\n",
        "def transformer_block(inputs, num_heads=8, ff_dim=64, dropout=0.2):\n",
        "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)(inputs, inputs)\n",
        "    attn_output = Dropout(dropout)(attn_output)\n",
        "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
        "\n",
        "    ffn_output = Dense(ff_dim, activation='relu')(out1)\n",
        "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
        "    ffn_output = Dropout(dropout)(ffn_output)\n",
        "    out2 = LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
        "\n",
        "    return out2\n",
        "\n",
        "# Model Input\n",
        "inputs = Input(shape=(1, X_train_reshaped.shape[2]))\n",
        "x = Conv1D(filters=32, kernel_size=1, activation='relu')(inputs)  # Added CNN layer for feature extraction\n",
        "x = BatchNormalization()(x)\n",
        "x = transformer_block(x)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dense(50, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "outputs = Dense(len(label_encoder.classes_), activation='softmax')(x)  # Binary classification\n",
        "\n",
        "# Build Model\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile model with improved optimizer and learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model with increased epochs and batch size\n",
        "model.fit(X_train_reshaped, y_train, epochs=50, batch_size=64, validation_data=(X_test_reshaped, y_test))\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate(X_test_reshaped, y_test)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJ6kLpGsgRhI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67ff6c55-2fe7-4867-aff0-4cb86914434f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m18999/18999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 17ms/step - accuracy: 0.9935 - loss: 0.0228 - val_accuracy: 0.7940 - val_loss: 1.7062\n",
            "Epoch 2/10\n",
            "\u001b[1m18999/18999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 16ms/step - accuracy: 0.9991 - loss: 0.0054 - val_accuracy: 0.7939 - val_loss: 1.3847\n",
            "Epoch 3/10\n",
            "\u001b[1m18999/18999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 16ms/step - accuracy: 0.9991 - loss: 0.0047 - val_accuracy: 0.7939 - val_loss: 0.4354\n",
            "Epoch 4/10\n",
            "\u001b[1m18999/18999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 16ms/step - accuracy: 0.9995 - loss: 0.0028 - val_accuracy: 0.9788 - val_loss: 0.3632\n",
            "Epoch 5/10\n",
            "\u001b[1m18999/18999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 16ms/step - accuracy: 0.9996 - loss: 0.0017 - val_accuracy: 0.7939 - val_loss: 1.2525\n",
            "Epoch 6/10\n",
            "\u001b[1m18999/18999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 16ms/step - accuracy: 0.9996 - loss: 0.0017 - val_accuracy: 0.7941 - val_loss: 1.2634\n",
            "Epoch 7/10\n",
            "\u001b[1m18999/18999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 16ms/step - accuracy: 0.9996 - loss: 0.0020 - val_accuracy: 0.9268 - val_loss: 0.4521\n",
            "Epoch 8/10\n",
            "\u001b[1m18999/18999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 16ms/step - accuracy: 0.9997 - loss: 0.0019 - val_accuracy: 0.9788 - val_loss: 0.4258\n",
            "Epoch 9/10\n",
            "\u001b[1m18999/18999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 16ms/step - accuracy: 0.9999 - loss: 5.6583e-04 - val_accuracy: 0.9201 - val_loss: 0.4772\n",
            "Epoch 10/10\n",
            "\u001b[1m18999/18999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 16ms/step - accuracy: 0.9998 - loss: 8.3891e-04 - val_accuracy: 0.9790 - val_loss: 0.3706\n",
            "\u001b[1m9908/9908\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 3ms/step - accuracy: 0.9657 - loss: 0.6051\n",
            "Test Accuracy: 97.90%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D, Conv1D, BatchNormalization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "import pyshark\n",
        "import os\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "def extract_pcap_features(pcap_file, label):\n",
        "    cap = pyshark.FileCapture(pcap_file, display_filter=\"bluetooth\")\n",
        "    data = []\n",
        "\n",
        "    for pkt in cap:\n",
        "        try:\n",
        "            protocol = pkt.highest_layer\n",
        "            length = int(pkt.length)\n",
        "            timestamp = float(pkt.sniff_time.timestamp())\n",
        "\n",
        "            data.append([protocol, length, timestamp, label])\n",
        "        except AttributeError:\n",
        "            continue\n",
        "\n",
        "    cap.close()\n",
        "    return pd.DataFrame(data, columns=['Protocol', 'Length', 'Timestamp', 'Label'])\n",
        "\n",
        "# Define dataset paths\n",
        "data_folder = \"/content/Bluebugging/\"\n",
        "train_files = {\n",
        "    \"Benign\": os.path.join(data_folder, \"train\", \"Bluetooth_Benign_train.pcap\"),\n",
        "    \"Attack\": os.path.join(data_folder, \"train\", \"Bluetooth_DoS_train.pcap\")\n",
        "}\n",
        "test_files = {\n",
        "    \"Benign\": os.path.join(data_folder, \"test\", \"Bluetooth_Benign_test.pcap\"),\n",
        "    \"Attack\": os.path.join(data_folder, \"test\", \"Bluetooth_DoS_test.pcap\")\n",
        "}\n",
        "\n",
        "# Process train data\n",
        "data_train = pd.concat([\n",
        "    extract_pcap_features(train_files[\"Benign\"], \"Benign\"),\n",
        "    extract_pcap_features(train_files[\"Attack\"], \"Attack\")\n",
        "])\n",
        "\n",
        "# Process test data\n",
        "data_test = pd.concat([\n",
        "    extract_pcap_features(test_files[\"Benign\"], \"Benign\"),\n",
        "    extract_pcap_features(test_files[\"Attack\"], \"Attack\")\n",
        "])\n",
        "\n",
        "# Select relevant features\n",
        "features = ['Length', 'Timestamp']  # Numerical features\n",
        "label_column = 'Label'  # Classification target\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "data_train[label_column] = label_encoder.fit_transform(data_train[label_column])\n",
        "data_test[label_column] = label_encoder.transform(data_test[label_column])\n",
        "\n",
        "# Extract feature matrix and labels\n",
        "X_train = data_train[features].values\n",
        "y_train = data_train[label_column].values\n",
        "X_test = data_test[features].values\n",
        "y_test = data_test[label_column].values\n",
        "\n",
        "# Normalize features\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Reshape for Transformer (samples, timesteps, features)\n",
        "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
        "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
        "\n",
        "# Define Transformer block\n",
        "def transformer_block(inputs, num_heads=8, ff_dim=64, dropout=0.2):\n",
        "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)(inputs, inputs)\n",
        "    attn_output = Dropout(dropout)(attn_output)\n",
        "    out1 = LayerNormalization(epsilon=1e-6)(inputs + attn_output)\n",
        "\n",
        "    ffn_output = Dense(ff_dim, activation='relu')(out1)\n",
        "    ffn_output = Dense(inputs.shape[-1])(ffn_output)\n",
        "    ffn_output = Dropout(dropout)(ffn_output)\n",
        "    out2 = LayerNormalization(epsilon=1e-6)(out1 + ffn_output)\n",
        "\n",
        "    return out2\n",
        "\n",
        "# Model Input\n",
        "inputs = Input(shape=(1, X_train_reshaped.shape[2]))\n",
        "x = Conv1D(filters=32, kernel_size=1, activation='relu')(inputs)  # Added CNN layer for feature extraction\n",
        "x = BatchNormalization()(x)\n",
        "x = transformer_block(x)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dense(50, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "outputs = Dense(len(label_encoder.classes_), activation='softmax')(x)  # Binary classification\n",
        "\n",
        "# Build Model\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile model with improved optimizer and learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model with increased epochs and batch size\n",
        "model.fit(X_train_reshaped, y_train, epochs=10, batch_size=64, validation_data=(X_test_reshaped, y_test))\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate(X_test_reshaped, y_test)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}